{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5921f1-59cf-427d-a9c0-7738a52d6aa0",
   "metadata": {},
   "source": [
    "# Mastering Binary Classification Metrics: A Complete Guide to ML Model Evaluation\n",
    "\n",
    "## Understanding when accuracy isn't enough and how to properly evaluate your machine learning models\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One of the most critical yet often misunderstood aspects of machine learning is model evaluation. Building a model is only half the battle; understanding whether it actually works well is equally important. This comprehensive guide explores the evaluation metrics covered in Week 4 of the Machine Learning Zoomcamp, using a real-world customer churn prediction dataset from Kaggle.\n",
    "\n",
    "By the end of this notes, you'll understand why accuracy alone can be misleading and how to choose the right metrics for your classification problems.\n",
    "\n",
    "## The Problem with Accuracy: Why a Single Metric Isn't Enough\n",
    "\n",
    "### What is Accuracy?\n",
    "\n",
    "Accuracy is the most intuitive metric for classification models. It simply measures the fraction of correct predictions:\n",
    "\n",
    "```\n",
    "Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\n",
    "```\n",
    "\n",
    "Sounds straightforward, right? Let's see why it can be deceptive.\n",
    "\n",
    "### The Dummy Model Trap\n",
    "\n",
    "Consider a customer churn prediction scenario where 73% of customers don't churn and only 27% do. If we build a \"dummy model\" that simply predicts no one will churn (by setting the decision threshold to 1), we'd achieve 73% accuracy without any real intelligence!\n",
    "\n",
    "Now, imagine our carefully crafted logistic regression model achieves 80% accuracy. That's only a 7% improvement over doing nothing. This reveals the fundamental issue: **accuracy doesn't work well with imbalanced datasets**.\n",
    "\n",
    "### Understanding Class Imbalance\n",
    "\n",
    "Class imbalance occurs when one category significantly outnumbers the other in your dataset. In such cases, a model can achieve high accuracy by simply predicting the majority class most of the time, while completely failing to identify the minority class that might be more important.\n",
    "\n",
    "This is why we need more sophisticated metrics.\n",
    "\n",
    "## The Confusion Matrix: Breaking Down Model Predictions\n",
    "\n",
    "The confusion matrix is a powerful tool that breaks down all possible outcomes of a binary classifier into four categories:\n",
    "\n",
    "### The Four Categories\n",
    "\n",
    "**Positive Class** (Prediction: Customer WILL churn)\n",
    "- **True Positive (TP)**: Correctly predicted churn\n",
    "- **False Positive (FP)**: Incorrectly predicted churn (customer stayed)\n",
    "\n",
    "**Negative Class** (Prediction: Customer WILL NOT churn)\n",
    "- **True Negative (TN)**: Correctly predicted no churn\n",
    "- **False Negative (FN)**: Incorrectly predicted no churn (customer left)\n",
    "\n",
    "### Confusion Matrix Structure\n",
    "\n",
    "```\n",
    "                    Predictions\n",
    "                Negative    Positive\n",
    "Actual Negative    TN          FP\n",
    "       Positive    FN          TP\n",
    "```\n",
    "\n",
    "This simple table unlocks a wealth of information about model performance and leads us to more nuanced metrics.\n",
    "\n",
    "## Precision and Recall: Understanding Different Types of Errors\n",
    "\n",
    "### Precision: Quality of Positive Predictions\n",
    "\n",
    "Precision answers the question: \"Of all the customers we predicted would churn, how many actually did?\"\n",
    "\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "**Mnemonic**: Precision is about **Predictions** — from the predicted positives, how many did we get right?\n",
    "\n",
    "High precision means when your model predicts churn, it's usually correct. This is crucial when false alarms are costly, such as when you're offering retention incentives to customers you think will leave.\n",
    "\n",
    "### Recall: Coverage of Actual Positives\n",
    "\n",
    "Recall (also called Sensitivity or True Positive Rate) answers: \"Of all the customers who actually churned, how many did we catch?\"\n",
    "\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "**Mnemonic**: Recall is about **Reality** — from the real positives, how many did we predict right?\n",
    "\n",
    "High recall means your model catches most of the churners, even if it makes some false alarms along the way. This matters when missing a positive case is very costly.\n",
    "\n",
    "### The Precision-Recall Tradeoff\n",
    "\n",
    "In the churn prediction example, the model achieved 67% precision and 54% recall. These metrics revealed problems that the 80% accuracy masked. The model was missing nearly half of the customers who would actually churn — a significant business problem!\n",
    "\n",
    "You can rarely optimize both precision and recall simultaneously. Increasing one typically decreases the other, requiring you to choose based on business priorities.\n",
    "\n",
    "## ROC Curves: Evaluating Performance Across All Thresholds\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "ROC (Receiver Operating Characteristic) curves originated during World War II for evaluating radar signal detection. Today, they're essential for assessing binary classifiers.\n",
    "\n",
    "### Understanding FPR and TPR\n",
    "\n",
    "**False Positive Rate (FPR)**\n",
    "```\n",
    "FPR = FP / (TN + FP)\n",
    "```\n",
    "The fraction of negatives incorrectly classified as positive. We want to minimize this.\n",
    "\n",
    "**True Positive Rate (TPR)** — Same as Recall\n",
    "```\n",
    "TPR = TP / (TP + FN)\n",
    "```\n",
    "The fraction of positives correctly identified. We want to maximize this.\n",
    "\n",
    "### Why ROC Curves Matter\n",
    "\n",
    "Most classifiers output probabilities, and we convert these to predictions using a threshold (commonly 0.5). But why 0.5? ROC curves show model performance across all possible thresholds, helping you:\n",
    "\n",
    "1. Understand the tradeoff between TPR and FPR\n",
    "2. Compare your model against random and ideal baselines\n",
    "3. Choose the optimal threshold for your specific use case\n",
    "\n",
    "### Plotting ROC Curves\n",
    "\n",
    "You can visualize ROC curves in two ways:\n",
    "- FPR and TPR vs. thresholds\n",
    "- TPR vs. FPR (more common)\n",
    "\n",
    "A random model produces a diagonal line from (0,0) to (1,1). An ideal model hugs the top-left corner. Your model should fall somewhere in between, ideally closer to the ideal model.\n",
    "\n",
    "## AUC: Summarizing ROC Performance in One Number\n",
    "\n",
    "### What is AUC?\n",
    "\n",
    "Area Under the ROC Curve (AUC or AUROC) condenses the entire ROC curve into a single metric between 0 and 1.\n",
    "\n",
    "- **AUC = 0.5**: Random model (no better than chance)\n",
    "- **AUC = 1.0**: Perfect model\n",
    "- **AUC = 0.8-0.9**: Generally considered good\n",
    "- **AUC < 0.7**: Often needs improvement\n",
    "\n",
    "### Probabilistic Interpretation\n",
    "\n",
    "AUC can be interpreted as the probability that your model ranks a randomly chosen positive example higher than a randomly chosen negative example. This makes it a robust metric even with class imbalance.\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_true, y_pred_proba)\n",
    "# or\n",
    "auc_score = auc(fpr, tpr)\n",
    "```\n",
    "\n",
    "## Cross-Validation: Getting Reliable Performance Estimates\n",
    "\n",
    "### The Problem with Single Train-Test Splits\n",
    "\n",
    "When you split your data once into training and test sets, you get a single performance estimate. But what if that particular split was lucky or unlucky? How confident are you in that number?\n",
    "\n",
    "### K-Fold Cross-Validation Explained\n",
    "\n",
    "Cross-validation provides a more robust performance estimate by:\n",
    "\n",
    "1. Dividing the training data into k partitions (folds)\n",
    "2. Training the model k times, each time using k-1 folds for training and 1 fold for validation\n",
    "3. Calculating the average performance and standard deviation across all folds\n",
    "\n",
    "### When to Use Cross-Validation\n",
    "\n",
    "**Use cross-validation when:**\n",
    "- Your dataset is small to moderate in size\n",
    "- You need to understand performance variability\n",
    "- You're tuning hyperparameters\n",
    "- You want a more reliable performance estimate\n",
    "\n",
    "**Use simple train-test split when:**\n",
    "- Your dataset is very large\n",
    "- Training is computationally expensive\n",
    "- You have a separate, large validation set\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "scores = []\n",
    "for train_idx, val_idx in tqdm(kfold.split(X)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_val, y_val)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Average: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n",
    "```\n",
    "\n",
    "The standard deviation tells you how stable your model is across different data subsets.\n",
    "\n",
    "## Choosing the Right Metric for Your Problem\n",
    "\n",
    "Different problems require different metrics. Here's a decision framework:\n",
    "\n",
    "### Use Accuracy When:\n",
    "- Classes are balanced\n",
    "- All errors are equally costly\n",
    "- You need a simple, interpretable metric\n",
    "\n",
    "### Use Precision When:\n",
    "- False positives are costly\n",
    "- Example: Spam detection (don't want legitimate emails marked as spam)\n",
    "\n",
    "### Use Recall When:\n",
    "- False negatives are costly\n",
    "- Example: Cancer detection (can't afford to miss positive cases)\n",
    "\n",
    "### Use F1 Score When:\n",
    "- You need a balance between precision and recall\n",
    "- Formula: `F1 = 2 × (Precision × Recall) / (Precision + Recall)`\n",
    "\n",
    "### Use AUC When:\n",
    "- You have class imbalance\n",
    "- You want a threshold-independent metric\n",
    "- You need to compare multiple models\n",
    "\n",
    "## Practical Tips and Best Practices\n",
    "\n",
    "### 1. Always Start with a Baseline\n",
    "Create a simple dummy model (like always predicting the majority class) to establish a baseline. Your real model should significantly outperform it.\n",
    "\n",
    "### 2. Look at Multiple Metrics\n",
    "Never rely on a single metric. Examine accuracy, precision, recall, and AUC together to get a complete picture.\n",
    "\n",
    "### 3. Visualize Your Results\n",
    "Plot confusion matrices, ROC curves, and precision-recall curves. Visual patterns often reveal insights that numbers alone don't.\n",
    "\n",
    "### 4. Consider Business Context\n",
    "The \"best\" metric depends on your business problem. Work with stakeholders to understand the cost of different types of errors.\n",
    "\n",
    "### 5. Use Cross-Validation for Hyperparameter Tuning\n",
    "When selecting model parameters, use cross-validation to avoid overfitting to a single validation set.\n",
    "\n",
    "## Key Python Libraries and Methods\n",
    "\n",
    "Here's a quick reference for the essential tools covered:\n",
    "\n",
    "```python\n",
    "# NumPy utilities\n",
    "np.linspace(0, 1, 50)  # Generate evenly spaced thresholds\n",
    "np.repeat([0, 1], [100, 50])  # Create arrays with repeated values\n",
    "\n",
    "# Scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Counting utilities\n",
    "from collections import Counter\n",
    "```\n",
    "\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "1. **Accuracy is misleading** with imbalanced datasets. Always check class distribution first.\n",
    "\n",
    "2. **Confusion matrices** break down model performance into four meaningful categories, enabling deeper analysis.\n",
    "\n",
    "3. **Precision and recall** address different business concerns. Choose based on which type of error is more costly.\n",
    "\n",
    "4. **ROC curves and AUC** provide threshold-independent evaluation and work well with imbalanced data.\n",
    "\n",
    "5. **Cross-validation** gives more reliable performance estimates and helps with hyperparameter tuning.\n",
    "\n",
    "6. **Always use multiple metrics** to get a complete picture of model performance.\n",
    "\n",
    "7. **Business context matters**. The best metric depends on the specific costs and benefits in your application.\n",
    "\n",
    "## Further Exploration\n",
    "\n",
    "To deepen your understanding, try these exercises:\n",
    "\n",
    "1. Calculate precision and recall for a dummy classifier that always predicts \"FALSE\"\n",
    "2. Plot precision-recall curves (similar to ROC curves) at different thresholds\n",
    "3. Calculate the area under the precision-recall curve\n",
    "4. Apply these metrics to other classification datasets\n",
    "5. Experiment with different decision thresholds to see how metrics change\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Machine Learning Zoomcamp](https://datatalks.club/blog/machine-learning-zoomcamp.html)\n",
    "- [Telco Customer Churn Dataset on Kaggle](https://www.kaggle.com/blastchar/telco-customer-churn)\n",
    "- [Complete Course Notebooks](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/04-evaluation/notebook.ipynb)\n",
    "- [Python Iterators and Generators](https://anandology.com/python-practice-book/iterators.html)\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Evaluating machine learning models properly is crucial for building systems that work in production. While accuracy might seem sufficient at first glance, this module demonstrates why understanding precision, recall, ROC curves, and cross-validation is essential for any data scientist or machine learning engineer.\n",
    "\n",
    "The metrics you choose directly impact business decisions, so invest time in understanding their nuances. Your stakeholders—and your models—will thank you.\n",
    "\n",
    "**What evaluation challenges have you faced in your projects? Share your experiences in the comments below!**\n",
    "\n",
    "---\n",
    "\n",
    "*These notes are based on Module 4 of the Machine Learning Zoomcamp course. If you're interested in learning more about practical machine learning, consider joining the course at DataTalks.Club.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
